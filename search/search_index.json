{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gigaflow","text":""},{"location":"#gigaflow-virtual-switch-gvs","title":"Gigaflow Virtual Switch (GvS)","text":"<p>Gigaflow is a multi-table cache architecture for the Open vSwitch (OVS) that captures pipeline-aware locality from the vSwitch pipelines to deliver significantly higher hit rate and lower end-to-end per-packet latency. Unlike traditional caches (e.g, Megaflow) that capture entire traversals as cache entries, Gigaflow caches sub-traversals that are shared among many flows to capture a cross-product rule space in the SmartNIC.</p>"},{"location":"#why-gigaflow","title":"Why Gigaflow?","text":""},{"location":"#minimizes-traffic-latency","title":"\ud83d\ude80 Minimizes Traffic Latency","text":"<ul> <li>Captures a cross-product rule space\u2014up to 450x bigger than Megaflow!</li> <li>Delivers up to 51% higher cache hit rate than traditional Megaflow cache</li> <li>Reduces cache misses by up to 90% while using 18% lesser cache entries</li> </ul>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Pipeline-Aware Locality: Captures locality from the vSwitch pipelines using disjointedness</li> <li>Longest Traversal Matching (LTM): Handles correctness in a multi-table lookup cache architecture </li> <li>Open vSwitch (OVS) Integration: Integrated in OVS as a new caching sub-system</li> </ul>"},{"location":"#technical-highlights","title":"\ud83d\udd27 Technical Highlights","text":"<ul> <li>Multi-table cache architecture enabling efficient matching across a significantly expanded rule space</li> <li>Effeciently captures pipeline-aware locality by maximizing sub-traversal level disjointedness</li> <li>SmartNIC offload available for Xilinx Alveo U250 Data Center Accelerator (coming soon!)</li> </ul>"},{"location":"#next-steps-with-gigaflow","title":"Next Steps with Gigaflow","text":""},{"location":"#quick-start","title":"\ud83d\udc49 Quick Start","text":"<ul> <li>Getting Started</li> <li>Installation Guide</li> <li>Usage Guide</li> </ul>"},{"location":"#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Technical Details</li> <li>Benchmarking Guide</li> </ul>"},{"location":"#research","title":"Research","text":"<p>Gigaflow was presented at ASPLOS'25. Read our paper:</p> <p>\ud83d\udcc4 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Here are your options:</p> <ul> <li>Review all the documentation</li> <li>Open an issue</li> <li>Review installation guide for setup help</li> </ul>"},{"location":"benchmarks/","title":"Gigaflow Benchmarking Guide","text":"<p>This guide explains how to evaluate Gigaflow against Megaflow cache using real-world vSwitch pipelines and traffic traces.</p>"},{"location":"benchmarks/#experiment-setup","title":"Experiment Setup","text":"<p>If you haven't completed the last step of installation, then you need to setup the experiment first. Inside the Ansible container, run the following command to install the datasets (pipelines and traffic traces) on the GVS and TGEN machines.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>This command will also install gvs and tgen along with all their dependencies on the respective machines.</p>"},{"location":"benchmarks/#option-1-run-all-experiments","title":"Option 1. Run All Experiments","text":"<p>To run all experiments (end-to-end and microbenchmarks) and collect logs, run the following command:</p> Ansible Container<pre><code>make run-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#option-2-end-to-end-evals","title":"Option 2. End-to-End Evals","text":"<p>To setup and run only end-to-end experiments:</p> Ansible Container<pre><code>make run-gvs-ee-experiment\n</code></pre>"},{"location":"benchmarks/#option-3-microbenchmarks","title":"Option 3. Microbenchmarks","text":"<p>To setup and run only microbenchmark experiments:</p> Ansible Container<pre><code>make run-gvs-bm-experiment\n</code></pre>"},{"location":"benchmarks/#option-4-custom-experiment","title":"Option 4. Custom Experiment","text":"<p>To setup and run a specific experiment (with a given locality, pipeline, and Gigaflow tables configuration), modify the following variables in vars/main.yml.</p>"},{"location":"benchmarks/#config-1-locality","title":"Config 1: Locality","text":"<p>The locality (high/low) to generate the correct traffic load. </p> vars/main.yml<pre><code>locality_dynamic:\n  current:\n    locality: \"high-locality\"\n</code></pre> <p>Choose an option from <code>locality_static</code>. The other available options are as following:</p> vars/main.yml<pre><code>locality_static:\n  all:\n    - locality: \"high-locality\"\n    - locality: \"low-locality\"\n</code></pre>"},{"location":"benchmarks/#config-2-vswitch-pipeline","title":"Config 2: vSwitch Pipeline","text":"<p>The pipeline to install in the vSwitch and send traffic for.</p> vars/main.yml<pre><code>pipelines_dynamic: \n  current: \n    name: \"cord-ofdpa\"\n    sub_path: \"cord/ofdpa\"\n</code></pre> <p>Choose an option from <code>pipelines_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>pipelines_static:\n  all:\n    - name: \"antrea-ovs\"\n      sub_path: \"antrea/ovs\"\n    - name: \"ovn-logical-switch\"\n      sub_path: \"ovn/logical-switch\"\n    - name: \"pisces-l2l3-acl\"\n      sub_path: \"pisces/l2l3-acl\"\n    - name: \"cord-ofdpa\"\n      sub_path: \"cord/ofdpa\"\n    - name: \"openflow-ttp-l2l3-acl\"\n      sub_path: \"openflow-ttp/l2l3-acl\"\n</code></pre>"},{"location":"benchmarks/#config-3-gigaflow-tables","title":"Config 3: Gigaflow Tables","text":"<p>The number of Gigaflow tables and entries in each of them.</p> vars/main.yml<pre><code>gigaflow_dynamic:\n  experiment: \"ee\" # this is just the name for the logs directory\n  options:\n      gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n</code></pre> <p>Choose an option from <code>gigaflow_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>gigaflow_static:\n  ee:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 32000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n  bm:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 2\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 3\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 5\n      gigaflow_max_entries: 100000\n</code></pre> <p>Once these variables are setup, run the following sequence of commands. </p> Ansible Container<pre><code>make resetup-tgen-scripts\nmake start-switch-gvs \nmake install-rules\nmake start-tgen\nmake stop-tgen\nmake uninstall-rules \nmake stop-switch-gvs\nmake collect-logs\n</code></pre>"},{"location":"benchmarks/#experiment-teardown","title":"Experiment Teardown","text":"<p>To stop the experiment and remove all installed components, run the following command:</p> Ansible Container<pre><code>make teardown-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Installation Instructions</li> <li>Usage Instructions</li> </ul>"},{"location":"contributing/","title":"Contributing to Gigaflow","text":"<p>We welcome contributions to Gigaflow! This guide explains how to contribute effectively to the project.</p>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch for the specific repo: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit: <pre><code>git commit -m \"Description of changes\"\n</code></pre></p> </li> <li> <p>Update documentation if needed:</p> <ul> <li>New features</li> <li>Configuration options</li> <li>Performance implications</li> </ul> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, please ensure:</p> <ul> <li>Documentation is updated</li> <li>Performance impact has been considered</li> <li>Backwards compatibility is maintained</li> <li>Code follows style guidelines</li> <li>All tests pass</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>Gigaflow version (or better yet commit number)</li> <li>System configuration</li> <li>Network environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Relevant logs</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>Use case</li> <li>Expected benefits</li> <li>Performance requirements</li> <li>Resource implications</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>We have several channels for communication:</p> <ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Email: Technical discussions and security reports</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with Gigaflow Virtual Switch (GvS).</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#testbed-setup","title":"Testbed Setup","text":"<p>To set up a testbed for Gigaflow and do all necessary installations, follow the steps provided in the Installation Guide. We also provide ready-made workloads (vSwitch pipelines, rulesets, and traffic traces) for benchmarking Gigaflow which will be installed automatically.</p>"},{"location":"getting-started/#bootstrap-gigaflow","title":"Bootstrap Gigaflow","text":"<p>If you haven't completed the last step of installation, then you need to setup the experiment first. Inside the Ansible container, run the following command to install the datasets (pipelines and traffic traces) on the GVS and TGEN machines.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>This command will also install gvs and tgen along with all their dependencies on the respective machines.</p>"},{"location":"getting-started/#run-all-benchmarks","title":"Run All Benchmarks","text":"<p>To run all experiments, including end-to-end and microbenchmarks, and collect performance logs, run the following command:</p> <p>Ansible Container<pre><code>make run-gvs-experiment\n</code></pre> See our benchmarking guide for various options to evaluate Gigaflow against Megaflow cache using real-world workloads.</p>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/#understanding-gigaflow","title":"Understanding Gigaflow","text":"<ul> <li>Check Usage Guide for detailed configuration options</li> <li>Read Technical Details to learn about Gigaflow's architecture</li> </ul>"},{"location":"getting-started/#optimizing-performance","title":"Optimizing Performance","text":"<ul> <li>Follow our Benchmarking Guide for in-depth performance evaluation</li> <li>Learn how to emulate high/low locality environments</li> <li>Comprehensively evaluate Gigaflow against traditional Megaflow cache</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the documentation pages linked above</li> <li>Review existing GitHub issues</li> <li>Open a new issue with a minimal example</li> </ol>"},{"location":"getting-started/#contributing","title":"Contributing","text":"<p>We welcome contributions to Gigaflow! Whether it's improving documentation, fixing bugs, optimizing performance, or adding new features, your help is appreciated. Please check our Contributing Guide for guidelines on how to get started.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>All dependencies and their installation is managed via Ansible which we run through a <code>docker</code> container.  So, the only required dependency for this setup is <code>docker</code>.  Follow the steps specified at this link and then allow non-root users to use docker by following these steps.</p>"},{"location":"installation/#testbed-setup","title":"Testbed Setup","text":"<p>The physical resources required on each machine are shown in the following figure.</p> The testbed setup to run Gigaflow out-of-the-box <p>The testbed requires 3 machines: </p> <ul> <li><code>COLLECTOR</code> \u2192 to store rulesets/traces and collect logs</li> <li><code>GVS</code> \u2192 device-under-test to run <code>gvs</code></li> <li><code>TGEN</code> \u2192 to send/receive traffic</li> </ul> <p>Info</p> <p>These machines can also be VMs running on the same physical host. The experiments in the paper were performed where the Ansible orchestrator was running on the same machine as <code>gvs</code> but it could also run on the <code>tgen</code> or the collector machine. The collector can be the same VM/machine running <code>gvs</code> or <code>tgen</code>. </p>"},{"location":"installation/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: Intel Xeon Platinum 8358P (64 cores) @ 2.60GHz</li> <li>Memory: 512 GB RAM</li> <li>SmartNIC <ul> <li>Intel XL710 (10/40G), Dual-port (for software-only Gigaflow)</li> <li>Xilinx Alveo U250 FPGA (hardware offload, support coming soon!)</li> </ul> </li> </ul> <p>Tip</p> <p>The memory and CPU requirements might seem bloated because of the test setup used for experiments. You should be able to run with much fewer resources (e.g. 16 cores, 16GB RAM) as long as the Intel XL710 10/40G SmartNICs are available.</p>"},{"location":"installation/#workloads","title":"Workloads","text":"<p>To evaluate a virtual switch, you need some packet pipeline rulesets and matching traffic traces. We have provided a set of 5 real-world vSwitch pipelines, their corresponding rulesets and traffic traces that we used for benchmarking Gigaflow. The following are their detailed descriptions (more details in the paper).</p> Pipeline Description Tables Traversals OFD CORD\u2019s Openflow data plane abstraction (OFDPA) for HW/SW switch integration 10 5 PSC An example L2L3-ACL pipeline implemented for the Pisces paper 7 2 OLS OVN Logical Switch pipeline to manage logical virtual network topologies in OVS 30 23 ANT Antrea pipeline implementing networking and security for Kubernetes clusters 22 20 OTL Openflow Table Type Patterns (TTP) to configure L2L3-ACL policies using OVS 8 11"},{"location":"installation/#download","title":"Download","text":"<p>Example pipelines, their rulesets, and traffic traces used for benchmarking Gigaflow are publicly available via FigShare. Download and place them on the <code>COLLECTOR</code> machine as following:</p> <p>Step 1: create directory shell<pre><code>mkdir ~/Gigaflow\ncd ~/Gigaflow\n</code></pre></p> <p>Step 2: download the traffic traces and pipelines shell<pre><code># download the CAIDA traffic traces\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608875\"\n# .. and vSwitch pipelines\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608872\"\n</code></pre></p> <p>Step 3: unzip the downloaded files shell<pre><code>unzip Traffic-Locality.zip\nunzip vSwitch-Pipelines.zip\n</code></pre></p> <p>Step 4: rename downloaded directories to match the <code>vars/main.yml</code> file shell<pre><code>mv Traffic-Locality mini_profiles\nmv vSwitch-Pipelines ovs-pipelines\n</code></pre></p> <p>Now, we are ready to setup the orchestrator and install <code>gvs</code> and the traffic generator (<code>tgen</code>).</p>"},{"location":"installation/#orchestrator-setup","title":"Orchestrator Setup","text":"<p>You only need to setup the gigaflow-orchestrator repository that will bringup the testbed, install all dependencies (including <code>gvs</code> and traffic generator), and run the experiments.  The orchestration is enabled via Ansible which itself is provided as a docker container.</p> <p>Note</p> <p>All the steps from this point onwards must be run on your orchestrator machine. For our experiments, we used the <code>gvs</code> machine as our orchestrator but you can choose a different machine too as long as it has <code>docker</code> installed.</p> <p>Clone the gigaflow-orchestrator repository as following:</p> shell<pre><code>git clone https://github.com/gigaflow-vswitch/gigaflow-orchestrator.git\n</code></pre>"},{"location":"installation/#update-local-paths","title":"Update Local Paths","text":"<p>In this repository, modify the following variables in the vars/main.yml file:</p> vars/main.yml<pre><code>retrieve:\n  caida:\n    path: \"/home/&lt;username&gt;/Gigaflow/mini_profiles\"\n  pipelines:\n    path: \"/home/&lt;username&gt;/Gigaflow/ovs-pipelines\"\n  destination: \n    path: \"/tmp/{{ project.name }}/pipelines-and-traffic\"\n</code></pre> <p>Update only the <code>retrieve.caida.path</code> and <code>retrieve.pipelines.path</code> variables to point to a Gigaflow directory on the <code>COLLECTOR</code> machine.</p>"},{"location":"installation/#inventory-configurations","title":"Inventory Configurations","text":"<p>We use Ansible to orcherstrate all experiments using the three machines.  Therefore, we require <code>root</code> access to each of them.  To populate for each machine, update the inventory.ini file as following:</p> inventory.ini<pre><code>[NODES]\nTGEN ansible_host=&lt;tgen-ip&gt; ansible_user=&lt;tgen-username&gt; ansible_password=&lt;tgen-password&gt; ansible_sudo_pass=&lt;tgen-root-password&gt;\nGVS ansible_host=&lt;ovs-ip&gt; ansible_user=&lt;ovs-username&gt; ansible_password=&lt;ovs-password&gt; ansible_sudo_pass=&lt;ovs-root-password&gt;\n\n[STORAGE]\nCOLLECTOR ansible_host=&lt;collector-ip&gt; ansible_user=&lt;collector-username&gt; ansible_password=&lt;collector-password&gt; ansible_sudo_pass=&lt;collector-root-password&gt; ansible_ssh_user=&lt;collector-username&gt; ansible_ssh_pass=&lt;collector-root-password&gt;\n</code></pre> <p>To test if all machines are reachable, run the following command:</p> shell<pre><code>cd gigaflow-orchestrator\nmake ansible\n</code></pre> <p>This should start an <code>Ansible</code> docker container. Run the next commands from inside this container.</p> <p>Note</p> <p>Except for <code>make ansible</code>, all make targets must always be run from inside this Ansible docker container.</p> Ansible Container<pre><code>make ping\n</code></pre> <p>This should be successful and return something like this:</p> <pre><code>root@nga2-vm2:/workdir# make ping\nansible all -m ping\nCOLLECTOR | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nGVS | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nTGEN | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>This means all machines are reachable and you can now proceed to the next step.</p>"},{"location":"installation/#gvs-and-tgen-installation","title":"GvS and TGen Installation","text":"<p>Now, you can install gvs and tgen along with all their dependencies as following:</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>You can also install them separately as following:</p> Ansible Container<pre><code>make install-dataset\nmake install-gvs\nmake install-tgen\n</code></pre> <p>At this point, you have succesfully installed <code>gvs</code> on the <code>GVS</code> machine and <code>tgen</code> on the <code>TGEN</code> machine. You have also retrieved and placed your pipelines and traffic traces on both machines and now you are ready to start running experiments!</p>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<pre><code>gigaflow-orchestrator/\n\u251c\u2500\u2500 roles/          # Ansible roles for all components needed to run GvS experiments\n\u2502   \u251c\u2500\u2500 collector/\n\u2502   \u251c\u2500\u2500 dpdk/\n\u2502   \u251c\u2500\u2500 gvs/\n\u2502   \u251c\u2500\u2500 logging/\n\u2502   \u251c\u2500\u2500 retrieve/\n\u2502   \u251c\u2500\u2500 rules/\n\u2502   \u2514\u2500\u2500 tgen/\n\u251c\u2500\u2500 scripts/        \n\u251c\u2500\u2500 vars/           # Experiment variables\n\u251c\u2500\u2500 inventory.ini   # Ansible inventory file\n\u251c\u2500\u2500 ansible.cfg     # Ansible configuration file\n\u251c\u2500\u2500 Makefile        # Makefile for the ansible playbook targets\n\u251c\u2500\u2500 gvs.yml         # top-level gvs ansible playbook\n\u251c\u2500\u2500 tgen.yml        # top-level tgen ansible playbook\n\u2514\u2500\u2500 ...             # other top-level ansible playbooks\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>For detailed instructions on using Gigaflow in your testbed, please refer to our usage guide.</p> <p>To evaluate Gigaflow performance against Megaflow cache with real-world workloads, please refer to our benchmarking guide.</p>"},{"location":"installation/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Performance Benchmarks</li> <li>Usage Instructions</li> </ul>"},{"location":"publications/","title":"Publications","text":"<p>ASPLOS 2025 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>Hot Chips 2024 Gigaflow: A Smart Cache for a SmartNIC! Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>SIGCOMM CCR 2023 The Slow Path Needs an Accelerator Too! Annus Zulfiqar, Ben Pfaff, William Tu, Gianni Antichi, Muhammad Shahbaz</p>"},{"location":"publications/#team","title":"Team","text":"<ul> <li>Annus Zulfiqar (University of Michigan)</li> <li>Ali Imran (University of Michigan)</li> <li>Venkat Kunaparaju (Purdue University)</li> <li>Ben Pfaff (Feldera)</li> <li>Gianni Antichi (Politechnico di Milano)</li> <li>Muhammad Shahbaz (University of Michigan)</li> </ul>"},{"location":"publications/#citation","title":"Citation","text":"<p>Please cite this paper when using Gigaflow:</p> <pre><code>@inproceedings{zulfiqar2025gigaflow,\ntitle = {{Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs}},\nauthor = {Zulfiqar, Annus and Imran, Ali and Kunaparaju, Venkat and Pfaff, Ben and Antichi, Gianni and Shahbaz, Muhammad},\nbooktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},\nyear = {2025}\npublisher = {Association for Computing Machinery},\n}\n</code></pre>"},{"location":"publications/#technical-documentation","title":"Technical Documentation","text":"<p>We maintain detailed documentation about Gigaflow:</p> <ul> <li>Getting Started and Usage</li> <li>Architecture and Implementation</li> <li>Performance Analysis</li> </ul>"},{"location":"publications/#contact","title":"Contact","text":"<p>For research-related queries or collaborations:</p> <ul> <li>Email: zulfiqaa@umich.edu</li> </ul>"},{"location":"technical-details/","title":"Technical Details","text":"<p>This document describes the technical architecture and implementation details of Gigaflow.</p>"},{"location":"technical-details/#gigaflow-architecture","title":"Gigaflow Architecture","text":"<p>Gigaflow is built as a caching sub-system in the Open vSwitch.</p> Gigaflow cache in the Open vSwitch (OVS) <ul> <li>The input packet that misses the cache is sent to the userspace where its <code>flow</code> is extracted (header field set).</li> <li>The flow passes through the vSwitch pipeline and its traversal is collected.</li> <li>Then, the travesal mapper, based on disjointedness, captures pipeline-aware locality from this traversal and suggests suitable sub-traversals to be cached.</li> <li>Finally, Longest Traversal Matching (LTM) cache entries are generated and installed into the Gigaflow table.</li> </ul>"},{"location":"technical-details/#pipeline-aware-locality","title":"Pipeline-Aware Locality","text":"<p>The following figure shows an example of how pipeline-aware locality captures unseen traversals by caching shared sub-traversals. We smartly partition the traversal and determine suitable sub-traversals that have a high likelihood of being reused in the future by other flows.</p> An example of Gigaflow cache in action with three packets"},{"location":"technical-details/#disjointedness-property","title":"Disjointedness Property","text":"<p>Capturing pipeline-Aware locality is expensive (see the paper) but we observe the following: disjoint sub-traversals maximize the cross-product rule space in Gigaflow tables.  We implement a dynamic program to find disjoint sub-traversals in a given traversal and this algorithm processes the traversal of each cache miss without looking at how previous cache entries were generated.</p> Gigaflow captures pipeline-aware locality by capitalizing on field-level disjointedness in traversals"},{"location":"technical-details/#mapping-traversals-to-gigaflow","title":"Mapping Traversals to Gigaflow","text":""},{"location":"technical-details/#traversal-processing","title":"Traversal Processing","text":"<p>The map_traversal_to_gigaflow function is responsible for mapping traversals to Gigaflow tables. It performs the following steps:</p> <ol> <li>Cleanup: Cleans up mapping context before starting the mapping process.</li> <li>Find Available Tables: Determines available Gigaflow tables with sufficient space for mapping.</li> <li>Disjointedness Exploration: Implements the dynamic program to explore disjointedness across sub-traversals.</li> <li>Compose and Map Wildcards: Composes and maps the wildcards as cache entries to the Gigaflow tables.</li> </ol> lib/dpif-netdev.c<pre><code>void\nmap_traversal_to_gigaflow(struct gigaflow_xlate_context *gf_xlate_ctx,\n                          struct gigaflow_mapping_context *gf_map_ctx,\n                          struct gigaflow_perf_stats *gf_stats,\n                          struct pmd_perf_stats *stats)\n{\n\n    /* no need to map if there was an error in translation \n       this happens for recursion too deep during resubmissions */\n    if (gf_xlate_ctx-&gt;xlate_error != XLATE_OK) {\n        gf_map_ctx-&gt;map_out = get_empty_mapper_out();\n        gf_map_ctx-&gt;mapped_cnt = 0;\n        return;\n    }\n\n    /* cleanup contexts before we start mapping \n       THIS MUST BE THE FIRST STEP WHEN WE START MAPPING */\n    cleanup_before_mapping(gf_xlate_ctx, gf_map_ctx);\n\n    /* 1. optimization passes to reduce the number of tables \n          to consider before calling path maximization DP\n        a. decoupling points in traversal?\n        b. header space locality opportunity?\n        c. explore vs exploit? */\n\n    /* generate bit wildcards and determine available tables for mapping */\n    generate_bit_wildcards(gf_xlate_ctx, gf_map_ctx);\n    find_available_tables_for_mapping(gf_map_ctx, gf_stats);\n\n    /* 2. find and assign a traversal mapping to Gigaflow tables \n       by maximizing added paths, coupling and other objectives */\n    uint64_t optimizer_cycles = cycles_counter_update(stats);\n    struct mapper_out *opt_map_out = maximize_optimality(gf_xlate_ctx, \n                                                         gf_map_ctx);\n    optimizer_cycles = cycles_counter_update(stats) - optimizer_cycles;\n    gf_perf_update_counter(gf_stats, GF_STAT_OPTIMIZER_CYCLES, optimizer_cycles);\n    gigaflow_mapping_ctx_assign_mapping(gf_xlate_ctx, gf_map_ctx, opt_map_out);\n\n    /* 3. compose and map the wildcards as Gigaflow entries to the accel */\n    uint64_t compose_cycles = cycles_counter_update(stats);\n    compose_gigaflows(gf_xlate_ctx, gf_map_ctx);\n    compose_cycles = cycles_counter_update(stats) - compose_cycles;\n    gf_perf_update_counter(gf_stats, GF_STAT_COMPOSITION_CYCLES, compose_cycles);\n\n    /* 4. accept this mapping iff masks are not exceeded in any Gigaflow table */\n    accept_mapping_if_masks_within_limits(gf_map_ctx, gf_stats);\n\n    const bool estimate_flow_space = \n        gf_map_ctx-&gt;gf_config-&gt;estimate_flow_space;\n    if (estimate_flow_space) {\n        /* 5. add this new mapping to our unique mappings */\n        gigaflow_state_add_new_mapping(gf_xlate_ctx, gf_map_ctx, gf_stats);\n    }\n}\n</code></pre>"},{"location":"technical-details/#exploring-sub-traversals","title":"Exploring Sub-Traversals","text":"<p>The function maximize_optimality_dp implements the disjoint partitioning algorithm as a dynamic program to find the optimal mapping of traversals to Gigaflow tables.  It uses memoization to store intermediate results and avoid redundant calculations.</p> lib/mapper.c<pre><code>struct mapper_out* \nmaximize_optimality_dp(struct mapper_memo *memo,\n                       struct gigaflow_xlate_context *gf_xlate_ctx,\n                       struct gigaflow_mapping_context *gf_map_ctx,\n                       int t_start, int t_end, int g_start, int g_end)\n{\n    /* check memo before making any calls */\n    struct dp_memo_entry *dp_cache;\n    dp_cache = search_dp_memo(memo, t_start, t_end, g_start, g_end);\n    if (dp_cache) {\n        return dp_cache-&gt;map_out;\n    }\n    struct mapper_out *map_out = get_empty_mapper_out();\n    /* base case - 1: \n       no more traversal tables to map? */\n    if (t_end == -1) {\n        map_out-&gt;score = 0;\n        // nothing to map..\n        return map_out;\n    }\n    /* base case - 2:\n       only one Gigaflow table left?\n       map remaining traversal tables to this Gigaflow table */\n    if (g_end == 0) {\n        /* get actual Gigaflow table ID before measuring optimality\n           or updating a mapping on that table */\n        int gf_table_id = gf_map_ctx-&gt;available_tables[g_end];\n        int score = get_mapping_optimality(memo, gf_xlate_ctx, gf_map_ctx, \n                                           t_start+1, t_end, t_end+1, \n                                           gf_table_id);\n        map_out-&gt;score = score;\n        // update this path mapping in mapper_out\n        mapper_out_update_mapping(map_out, t_start+1, t_end, t_end+1, \n                                  gf_table_id);\n        // insert dp result into memo before returning\n        insert_into_dp_memo(memo, map_out, t_start, t_end, \n                            g_start, g_end, score);\n        return map_out;\n    }\n    // recursive calls\n    struct mapper_out *dp_map_out;\n    int mapped_score = 0, new_score = 0, max_score = 0;\n    for (int t_i=t_start; t_i&lt;=t_end; t_i++) {\n        dp_map_out = maximize_optimality_dp(memo, gf_xlate_ctx, gf_map_ctx, \n                                            t_start, t_i, g_start, \n                                            g_end-1);\n        /* get actual Gigaflow table ID before measuring optimality\n           or updating a mapping on that table */\n        int gf_table_id = gf_map_ctx-&gt;available_tables[g_end];\n        mapped_score = get_mapping_optimality(memo, gf_xlate_ctx, gf_map_ctx, \n                                              t_i+1, t_end, t_end+1, \n                                              gf_table_id);\n        new_score = dp_map_out-&gt;score + mapped_score;\n        /* found a better or equally optimal solution? \n           replace the prior mapping for this table */\n        if (new_score &gt; max_score) {\n            // free previous map_out and update to new best\n            mapper_out_destroy(map_out);\n            // update this path mapping in mapper_out iff t_i+1 &lt;= t_end\n            if (t_i + 1 &lt;= t_end) {\n                mapper_out_update_mapping(dp_map_out, t_i+1, t_end, \n                                          t_end+1, gf_table_id);\n            }\n            dp_map_out-&gt;score = new_score;\n            map_out = dp_map_out;\n            max_score = new_score;\n        } else {\n            // this solution we tried is not any better\n            mapper_out_destroy(dp_map_out);\n        }\n    }\n    // insert dp result into memo before returning\n    insert_into_dp_memo(memo, map_out, t_start, t_end, \n                        g_start, g_end, max_score);\n    return map_out;\n}\n</code></pre>"},{"location":"technical-details/#evaluating-a-sub-traversal","title":"Evaluating a Sub-Traversal","text":"<p>The function that evaluates one sub-traversal to see if all tables overlap is called get_coupling.</p> lib/mapper.c<pre><code>int \nget_coupling(struct gigaflow_mapping_context *gf_map_ctx,\n             int t_start, int t_end)\n{\n    const uint32_t coupling_base_score = \n        gf_map_ctx-&gt;gf_config-&gt;coupling_base_score;\n    int coupling = 0; // just one wildcard is not a coupling\n    uint16_t next_wc_bits = EMPTY_WC;\n    uint16_t curr_wc_bits = gf_map_ctx-&gt;bit_wildcards[t_start];\n    for (int i=t_start+1; i&lt;=t_end; i++) {\n        next_wc_bits = gf_map_ctx-&gt;bit_wildcards[i];\n        /* if either of current or next are empty wildcards OR \n           if not, have common bits, subsume into current and move on */\n        if ((curr_wc_bits == EMPTY_WC) || (next_wc_bits == EMPTY_WC)\n            || (curr_wc_bits &amp; next_wc_bits)) {\n            curr_wc_bits |= next_wc_bits;\n            coupling += coupling_base_score; // better coupling\n        } else {\n            /* found decoupling within this range; bad combination */\n            return 0;\n        }\n    }\n    return coupling;\n}\n</code></pre>"},{"location":"technical-details/#putting-it-all-together","title":"Putting It All Together","text":"<p>For a given traversal, our disjoint partitioning algorithm tries all sub-traversal combinations and evaluates them for disjointedness using the get_coupling function. The sub-traversal combination with maximum sum of disjointedness is selected for mapping to the Gigaflow tables.</p>"},{"location":"usage/","title":"Using Gigaflow","text":"<p>This guide explains how to use Gigaflow as a new caching sub-system integrated in the Open vSwitch (GvS).</p>"},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>Before using Gigaflow, ensure you have:</p> The high-level workflow to evaluate Gigaflow (GvS)"},{"location":"usage/#running-gigaflow-and-performance-evaluation","title":"Running Gigaflow and Performance Evaluation","text":"<p>To evaluate performance:</p> <ol> <li>Follow our benchmarking guide</li> <li>Use provided workloads and scripts to emulate high/low locality environments</li> <li>Benchmark Gigaflow against Megaflow cache using real-world vSwitch pipelines and traffic traces</li> </ol>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See Installation Instructions for setup details</li> <li>Review Technical Details for architecture information</li> <li>Check Benchmarks for performance evaluation guide</li> </ul>"}]}