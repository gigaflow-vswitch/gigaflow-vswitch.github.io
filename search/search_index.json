{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gigaflow","text":""},{"location":"#gigaflow-virtual-switch-gvs","title":"Gigaflow Virtual Switch (GvS)","text":"<p>Gigaflow is a multi-table cache architecture for the Open vSwitch (OVS) that captures pipeline-aware locality from the vSwitch pipelines to deliver significantly higher hit rate and lower end-to-end per-packet latency. Unlike traditional caches (e.g, Megaflow) that capture entire traversals as cache entries, Gigaflow caches sub-traversals that are shared among many flows to capture a cross-product rule space in the SmartNIC.</p>"},{"location":"#why-gigaflow","title":"Why Gigaflow?","text":""},{"location":"#minimizes-traffic-latency","title":"\ud83d\ude80 Minimizes Traffic Latency","text":"<ul> <li>Captures a cross-product rule space\u2014up to 450x bigger than Megaflow!</li> <li>Delivers up to 51% higher cache hit rate than traditional Megaflow cache</li> <li>Reduces cache misses by up to 90% while using 18% lesser cache entries</li> </ul>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Pipeline-Aware Locality: Captures locality from the vSwitch pipelines using disjointedness</li> <li>Longest Traversal Matching (LTM): Handles correctness in a multi-table lookup cache architecture </li> <li>Open vSwitch (OVS) Integration: Integrated in OVS as a new caching sub-system</li> </ul>"},{"location":"#technical-highlights","title":"\ud83d\udd27 Technical Highlights","text":"<ul> <li>Multi-table cache architecture enabling efficient matching across a significantly expanded rule space</li> <li>Effeciently captures pipeline-aware locality by maximizing sub-traversal level disjointedness</li> <li>SmartNIC offload available for Xilinx Alveo U250 Data Center Accelerator (coming soon!)</li> </ul>"},{"location":"#next-steps-with-gigaflow","title":"Next Steps with Gigaflow","text":""},{"location":"#quick-start","title":"\ud83d\udc49 Quick Start","text":"<ul> <li>Getting Started</li> <li>Installation Guide</li> <li>Usage Guide</li> </ul>"},{"location":"#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Technical Details</li> <li>Benchmarking Guide</li> </ul>"},{"location":"#research","title":"Research","text":"<p>Gigaflow was presented at ASPLOS'25. Read our paper:</p> <p>\ud83d\udcc4 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Here are your options:</p> <ul> <li>Review all the documentation</li> <li>Open an issue</li> <li>Review installation guide for setup help</li> </ul>"},{"location":"contributing/","title":"Contributing to Gigaflow","text":"<p>We welcome contributions to Gigaflow! This guide explains how to contribute effectively to the project.</p>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch for the specific repo: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit: <pre><code>git commit -m \"Description of changes\"\n</code></pre></p> </li> <li> <p>Update documentation if needed:</p> <ul> <li>New features</li> <li>Configuration options</li> <li>Performance implications</li> </ul> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, please ensure:</p> <ul> <li>Documentation is updated</li> <li>Performance impact has been considered</li> <li>Backwards compatibility is maintained</li> <li>Code follows style guidelines</li> <li>All tests pass</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>Gigaflow version (or better yet commit number)</li> <li>System configuration</li> <li>Network environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Relevant logs</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>Use case</li> <li>Expected benefits</li> <li>Performance requirements</li> <li>Resource implications</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>We have several channels for communication:</p> <ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Email: Technical discussions and security reports</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Dependencies</p> <p>All dependencies and their installation is managed via Ansible which we run through a <code>docker</code> container. The only required dependency for this setup is <code>docker</code>. Follow the steps specified at this link and then allow non-root users to use docker by following these steps.</p>"},{"location":"installation/#testbed-setup","title":"Testbed Setup","text":"The testbed setup for running Gigaflow <p>The testbed requires 3 machines: </p> <ul> <li><code>COLLECTOR</code> \u2192 to store rulesets/traces and collect logs</li> <li><code>GVS</code> \u2192 device-under-test to run <code>gvs</code></li> <li><code>TGEN</code> \u2192 to send/receive traffic</li> </ul>"},{"location":"installation/#hardware-requirements","title":"Hardware Requirements","text":"<p>The physical resources required on each machine are labeled on above figure.</p> <p>Note</p> <p>These three machines can also be VMs running on the same physical host. The experiments in the paper were performed where the Ansible orchestrator was running on the same machine as <code>gvs</code> but it could also run on the <code>tgen</code> or the collector machine. The collector can be the same VM/machine running <code>gvs</code> or <code>tgen</code>. Finally, the memory and CPU requirements might seem bloated because of the test setup used for experiments. You should be able to run with much fewer resources (e.g. 16 cores, 16GB RAM) as long as the Intel XL710 10/40G NICs are available.</p>"},{"location":"installation/#workloads","title":"Workloads","text":"<p>To evaluate a virtual switch, you need some packet pipeline rulesets and matching traffic traces. We have provided a set of 5 real-world vSwitch pipelines, their corresponding rulesets and traffic traces that we used for benchmarking Gigaflow. The following are their detailed descriptions (more details in the paper.</p> Pipeline Description Tables Traversals OFD CORD\u2019s Openflow data plane abstraction (OFDPA) for HW/SW switch integration 10 5 PSC An example L2L3-ACL pipeline implemented for the Pisces paper 7 2 OLS OVN Logical Switch pipeline to manage logical virtual network topologies in OVS 30 23 ANT Antrea pipeline implementing networking and security for Kubernetes clusters 22 20 OTL Openflow Table Type Patterns (TTP) to configure L2L3-ACL policies using OVS 8 11 <p>These pipelines, their rulesets, and traffic traces used for benchmarking Gigaflow are publicly available via FigShare. Download them and place them on the <code>COLLECTOR</code> machine as following:</p> COLLECTOR<pre><code># step 1: create directory\nmkdir ~/Gigaflow\ncd ~/Gigaflow\n\n# step 2: get traffic traces\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608875\"\n# .. and vSwitch pipelines\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608872\"\n\n# step 3: extract\nunzip Traffic-Locality.zip\nunzip vSwitch-Pipelines.zip\n</code></pre> <p>Now, we are ready to setup the orchestrator and install <code>gvs</code> and the traffic generator (<code>tgen</code>).</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>You only need to setup the gigaflow-orchestrator repository that will bringup the testbed, install all dependencies (including <code>gvs</code> and traffic generator), and run the experiments.  The orchestration is enabled via Ansible which itself is provided as a docker container.</p> <pre><code>git clone https://github.com/gigaflow-vswitch/Gigaflow-Artifact-ASPLOS2025/\n</code></pre> <p>Now, you can install gvs and tgen alongside all their dependencies by running the following command from the <code>Gigaflow-Artifact-ASPLOS2025</code> directory:</p> shell<pre><code>cd Gigaflow-Artifact-ASPLOS2025\nmake ansible\n</code></pre> <p>This should start an <code>Ansible</code> docker container. Run the next commands from inside this container.</p> <p>Tip</p> <p>Except for <code>make ansible</code>, all make targets must always be run from within the Ansible docker container.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>You can also install them separately as following:</p> Ansible Container<pre><code>make install-dataset\nmake install-gvs\nmake install-tgen\n</code></pre>"},{"location":"installation/#inventory-configurations","title":"Inventory Configurations","text":"<p>We use Ansible to orcherstrate all experiments using these three machines. Therefore, we require <code>root</code> access to each of them. To populate for each machine, update the <code>inventory.ini</code> file as following:</p> inventory.ini<pre><code>[NODES]\nTGEN ansible_host=&lt;tgen-ip&gt; ansible_user=&lt;tgen-username&gt; ansible_password=&lt;tgen-password&gt; ansible_sudo_pass=&lt;tgen-root-password&gt;\nGVS ansible_host=&lt;ovs-ip&gt; ansible_user=&lt;ovs-username&gt; ansible_password=&lt;ovs-password&gt; ansible_sudo_pass=&lt;ovs-root-password&gt;\n\n[STORAGE]\nCOLLECTOR ansible_host=&lt;collector-ip&gt; ansible_user=&lt;collector-username&gt; ansible_password=&lt;collector-password&gt; ansible_sudo_pass=&lt;collector-root-password&gt; ansible_ssh_user=&lt;collector-username&gt; ansible_ssh_pass=&lt;collector-root-password&gt;\n</code></pre>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<pre><code>gigaflow-orchestrator/\n\u251c\u2500\u2500 roles/          # Ansible roles for all components needed to run GvS experiments\n\u2502   \u251c\u2500\u2500 collector/\n\u2502   \u251c\u2500\u2500 dpdk/\n\u2502   \u251c\u2500\u2500 gvs/\n\u2502   \u251c\u2500\u2500 logging/\n\u2502   \u251c\u2500\u2500 retrieve/\n\u2502   \u251c\u2500\u2500 rules/\n\u2502   \u2514\u2500\u2500 tgen/\n\u251c\u2500\u2500 scripts/        \n\u251c\u2500\u2500 vars/           # Experiment variables\n\u251c\u2500\u2500 inventory.ini   # Ansible inventory file\n\u251c\u2500\u2500 ansible.cfg     # Ansible configuration file\n\u251c\u2500\u2500 Makefile        # Makefile for the ansible playbook targets\n\u251c\u2500\u2500 gvs.yml         # top-level gvs yaml playbook\n\u251c\u2500\u2500 tgen.yml        # top-level tgen yaml playbook\n\u2514\u2500\u2500 ...             # other top-level yaml playbooks\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>For detailed instructions on using OptiReduce in your distributed training setup, please refer to our usage guide.</p> <p>To evaluate OptiReduce's performance and compare different communication schemes in your environment, please refer to our benchmarking guide.</p>"},{"location":"installation/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Performance Benchmarks</li> <li>Usage Instructions</li> </ul>"},{"location":"publications/","title":"Publications","text":"<p>ASPLOS 2025 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>Hot Chips 2024 Gigaflow: A Smart Cache for a SmartNIC! Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>SIGCOMM CCR 2023 The Slow Path Needs an Accelerator Too! Annus Zulfiqar, Ben Pfaff, William Tu, Gianni Antichi, Muhammad Shahbaz</p>"},{"location":"publications/#team","title":"Team","text":"<ul> <li>Annus Zulfiqar (University of Michigan)</li> <li>Ali Imran (University of Michigan)</li> <li>Venkat Kunaparaju (Purdue University)</li> <li>Ben Pfaff (Feldera)</li> <li>Gianni Antichi (Politechnico di Milano)</li> <li>Muhammad Shahbaz (University of Michigan)</li> </ul>"},{"location":"publications/#citation","title":"Citation","text":"<p>Please cite this paper when using Gigaflow:</p> <pre><code>@inproceedings{zulfiqar2025gigaflow,\ntitle = {{Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs}},\nauthor = {Zulfiqar, Annus and Imran, Ali and Kunaparaju, Venkat and Pfaff, Ben and Antichi, Gianni and Shahbaz, Muhammad},\nbooktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},\nyear = {2025}\npublisher = {Association for Computing Machinery},\n}\n</code></pre>"},{"location":"publications/#technical-documentation","title":"Technical Documentation","text":"<p>We maintain detailed documentation about Gigaflow:</p> <ul> <li>Getting Started and Usage</li> <li>Architecture and Implementation</li> <li>Performance Analysis</li> </ul>"},{"location":"publications/#contact","title":"Contact","text":"<p>For research-related queries or collaborations:</p> <ul> <li>Email: zulfiqaa@umich.edu</li> </ul>"},{"location":"technical-details/","title":"Technical Details","text":"<p>This document describes the technical architecture and implementation details of Gigaflow.</p>"},{"location":"technical-details/#architecture-overview","title":"Architecture Overview","text":"<p>Gigaflow is built as a caching sub-system in the Open vSwitch.</p>"},{"location":"technical-details/#gigaflow-components","title":"Gigaflow Components","text":"Figure 1: Gigaflow cache in the Open vSwitch (OVS)"}]}