{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gigaflow","text":""},{"location":"#gigaflow-virtual-switch-gvs","title":"Gigaflow Virtual Switch (GvS)","text":"<p>Gigaflow is a multi-table cache architecture for the Open vSwitch (OVS) that captures pipeline-aware locality from the vSwitch pipelines to deliver significantly higher hit rate and lower end-to-end per-packet latency. Unlike traditional caches (e.g, Megaflow) that capture entire traversals as cache entries, Gigaflow caches sub-traversals that are shared among many flows to capture a cross-product rule space in the SmartNIC.</p>"},{"location":"#why-gigaflow","title":"Why Gigaflow?","text":""},{"location":"#minimizes-traffic-latency","title":"\ud83d\ude80 Minimizes Traffic Latency","text":"<ul> <li>Captures a cross-product rule space\u2014up to 450x bigger than Megaflow!</li> <li>Delivers up to 51% higher cache hit rate than traditional Megaflow cache</li> <li>Reduces cache misses by up to 90% while using 18% lesser cache entries</li> </ul>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Pipeline-Aware Locality: Captures locality from the vSwitch pipelines using disjointedness</li> <li>Longest Traversal Matching (LTM): Handles correctness in a multi-table lookup cache architecture </li> <li>Open vSwitch (OVS) Integration: Integrated in OVS as a new caching sub-system</li> </ul>"},{"location":"#technical-highlights","title":"\ud83d\udd27 Technical Highlights","text":"<ul> <li>Multi-table cache architecture enabling efficient matching across a significantly expanded rule space</li> <li>Effeciently captures pipeline-aware locality by maximizing sub-traversal level disjointedness</li> <li>SmartNIC offload available for Xilinx Alveo U250 Data Center Accelerator (coming soon!)</li> </ul>"},{"location":"#next-steps-with-gigaflow","title":"Next Steps with Gigaflow","text":""},{"location":"#quick-start","title":"\ud83d\udc49 Quick Start","text":"<ul> <li>Getting Started</li> <li>Installation Guide</li> <li>Usage Guide</li> </ul>"},{"location":"#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Technical Details</li> <li>Benchmarking Guide</li> </ul>"},{"location":"#research","title":"Research","text":"<p>Gigaflow was presented at ASPLOS'25. Read our paper:</p> <p>\ud83d\udcc4 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Here are your options:</p> <ul> <li>Review all the documentation</li> <li>Open an issue</li> <li>Review installation guide for setup help</li> </ul>"},{"location":"benchmarks/","title":"Gigaflow Benchmarking Guide","text":"<p>This guide explains how to evaluate Gigaflow against Megaflow cache using real-world vSwitch pipelines and traffic traces.</p>"},{"location":"benchmarks/#experiment-setup","title":"Experiment Setup","text":"<p>If you haven't completed the last step of installation, then you need to setup the experiment first. Inside the Ansible container, run the following command to install the datasets (pipelines and traffic traces) on the GVS and TGEN machines.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>This command will also install gvs and tgen along with all their dependencies on the respective machines.</p>"},{"location":"benchmarks/#option-1-run-all-experiments","title":"Option 1. Run All Experiments","text":"<p>To run all experiments (end-to-end and microbenchmarks) and collect logs, run the following command:</p> Ansible Container<pre><code>make run-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#option-2-end-to-end-evals","title":"Option 2. End-to-End Evals","text":"<p>To setup and run only end-to-end experiments:</p> Ansible Container<pre><code>make run-gvs-ee-experiment\n</code></pre>"},{"location":"benchmarks/#option-3-microbenchmarks","title":"Option 3. Microbenchmarks","text":"<p>To setup and run only microbenchmark experiments:</p> Ansible Container<pre><code>make run-gvs-bm-experiment\n</code></pre>"},{"location":"benchmarks/#option-4-custom-experiment","title":"Option 4. Custom Experiment","text":"<p>To setup and run a specific experiment (with a given locality, pipeline, and Gigaflow tables configuration), modify the following variables in <code>vars/main.yml</code>.</p>"},{"location":"benchmarks/#config-1-locality","title":"Config 1: Locality","text":"<p>The locality (high/low) to generate the correct traffic load. </p> vars/main.yml<pre><code>locality_dynamic:\n  current:\n    locality: \"high-locality\"\n</code></pre> <p>Choose an option from <code>locality_static</code>. The other available options are as following:</p> vars/main.yml<pre><code>locality_static:\n  all:\n    - locality: \"high-locality\"\n    - locality: \"low-locality\"\n</code></pre>"},{"location":"benchmarks/#config-2-vswitch-pipeline","title":"Config 2: vSwitch Pipeline","text":"<p>The pipeline to install in the vSwitch and send traffic for.</p> vars/main.yml<pre><code>pipelines_dynamic: \n  current: \n    name: \"cord-ofdpa\"\n    sub_path: \"cord/ofdpa\"\n</code></pre> <p>Choose an option from <code>pipelines_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>pipelines_static:\n  all:\n    - name: \"antrea-ovs\"\n      sub_path: \"antrea/ovs\"\n    - name: \"ovn-logical-switch\"\n      sub_path: \"ovn/logical-switch\"\n    - name: \"pisces-l2l3-acl\"\n      sub_path: \"pisces/l2l3-acl\"\n    - name: \"cord-ofdpa\"\n      sub_path: \"cord/ofdpa\"\n    - name: \"openflow-ttp-l2l3-acl\"\n      sub_path: \"openflow-ttp/l2l3-acl\"\n</code></pre>"},{"location":"benchmarks/#config-3-gigaflow-tables","title":"Config 3: Gigaflow Tables","text":"<p>The number of Gigaflow tables and entries in each of them.</p> vars/main.yml<pre><code>gigaflow_dynamic:\n  experiment: \"ee\" # this is just the name for the logs directory\n  options:\n      gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n</code></pre> <p>Choose an option from <code>gigaflow_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>gigaflow_static:\n  ee:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 32000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n  bm:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 2\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 3\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 5\n      gigaflow_max_entries: 100000\n</code></pre> <p>Once these variables are setup, run the following sequence of commands. </p> Ansible Container<pre><code>make resetup-tgen-scripts\nmake start-switch-gvs \nmake install-rules\nmake start-tgen\nmake stop-tgen\nmake uninstall-rules \nmake stop-switch-gvs\nmake collect-logs\n</code></pre>"},{"location":"benchmarks/#experiment-teardown","title":"Experiment Teardown","text":"<p>To stop the experiment and remove all installed components, run the following command:</p> Ansible Container<pre><code>make teardown-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Installation Instructions</li> <li>Usage Instructions</li> </ul>"},{"location":"contributing/","title":"Contributing to Gigaflow","text":"<p>We welcome contributions to Gigaflow! This guide explains how to contribute effectively to the project.</p>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch for the specific repo: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit: <pre><code>git commit -m \"Description of changes\"\n</code></pre></p> </li> <li> <p>Update documentation if needed:</p> <ul> <li>New features</li> <li>Configuration options</li> <li>Performance implications</li> </ul> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, please ensure:</p> <ul> <li>Documentation is updated</li> <li>Performance impact has been considered</li> <li>Backwards compatibility is maintained</li> <li>Code follows style guidelines</li> <li>All tests pass</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>Gigaflow version (or better yet commit number)</li> <li>System configuration</li> <li>Network environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Relevant logs</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>Use case</li> <li>Expected benefits</li> <li>Performance requirements</li> <li>Resource implications</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>We have several channels for communication:</p> <ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Email: Technical discussions and security reports</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with Gigaflow Virtual Switch (GvS).</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#1-setup-testbed","title":"1. Setup Testbed","text":"<p>To set up a testbed for Gigaflow and do all necessary installations, follow the steps provided in the Installation Guide. We also provide ready-made workloads (vSwitch pipelines, rulesets, and traffic traces) for benchmarking Gigaflow which will be installed automatically.</p>"},{"location":"getting-started/#2-benchmark-gigaflow","title":"2. Benchmark Gigaflow","text":"<p>See our benchmarking guide for various options to evaluate Gigaflow against Megaflow cache using real-world workloads.</p>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/#understanding-gigaflow","title":"Understanding Gigaflow","text":"<ul> <li>Check Usage Guide for detailed configuration options</li> <li>Read Technical Details to learn about Gigaflow's architecture</li> </ul>"},{"location":"getting-started/#optimizing-performance","title":"Optimizing Performance","text":"<ul> <li>Follow our Benchmarking Guide to evaluate performance</li> <li>Learn how to emulate high/low locality environments</li> <li>Comprehensively evaluate Gigaflow against traditional Megaflow cache</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the documentation pages linked above</li> <li>Review existing GitHub issues</li> <li>Open a new issue with a minimal example</li> </ol>"},{"location":"getting-started/#contributing","title":"Contributing","text":"<p>We welcome contributions to Gigaflow! Whether it's improving documentation, fixing bugs, optimizing performance, or adding new features, your help is appreciated. Please check our Contributing Guide for guidelines on how to get started.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>All dependencies and their installation is managed via Ansible which we run through a <code>docker</code> container.  So, the only required dependency for this setup is <code>docker</code>.  Follow the steps specified at this link and then allow non-root users to use docker by following these steps.</p>"},{"location":"installation/#testbed-setup","title":"Testbed Setup","text":"The testbed setup to run Gigaflow out-of-the-box <p>The testbed requires 3 machines: </p> <ul> <li><code>COLLECTOR</code> \u2192 to store rulesets/traces and collect logs</li> <li><code>GVS</code> \u2192 device-under-test to run <code>gvs</code></li> <li><code>TGEN</code> \u2192 to send/receive traffic</li> </ul>"},{"location":"installation/#hardware-requirements","title":"Hardware Requirements","text":"<p>The physical resources required on each machine are labeled on above figure.</p> <p>Tip</p> <p>These three machines can also be VMs running on the same physical host. The experiments in the paper were performed where the Ansible orchestrator was running on the same machine as <code>gvs</code> but it could also run on the <code>tgen</code> or the collector machine. The collector can be the same VM/machine running <code>gvs</code> or <code>tgen</code>. Finally, the memory and CPU requirements might seem bloated because of the test setup used for experiments. You should be able to run with much fewer resources (e.g. 16 cores, 16GB RAM) as long as the Intel XL710 10/40G NICs are available.</p>"},{"location":"installation/#workloads","title":"Workloads","text":"<p>To evaluate a virtual switch, you need some packet pipeline rulesets and matching traffic traces. We have provided a set of 5 real-world vSwitch pipelines, their corresponding rulesets and traffic traces that we used for benchmarking Gigaflow. The following are their detailed descriptions (more details in the paper).</p> Pipeline Description Tables Traversals OFD CORD\u2019s Openflow data plane abstraction (OFDPA) for HW/SW switch integration 10 5 PSC An example L2L3-ACL pipeline implemented for the Pisces paper 7 2 OLS OVN Logical Switch pipeline to manage logical virtual network topologies in OVS 30 23 ANT Antrea pipeline implementing networking and security for Kubernetes clusters 22 20 OTL Openflow Table Type Patterns (TTP) to configure L2L3-ACL policies using OVS 8 11"},{"location":"installation/#download","title":"Download","text":"<p>Example pipelines, their rulesets, and traffic traces used for benchmarking Gigaflow are publicly available via FigShare. Download and place them on the <code>COLLECTOR</code> machine as following:</p> <p>Step 1: create directory shell<pre><code>mkdir ~/Gigaflow\ncd ~/Gigaflow\n</code></pre></p> <p>Step 2: download the traffic traces and pipelines shell<pre><code># download the CAIDA traffic traces\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608875\"\n# .. and vSwitch pipelines\nwget --content-disposition \"https://figshare.com/ndownloader/files/52608872\"\n</code></pre></p> <p>Step 3: unzip the downloaded files shell<pre><code>unzip Traffic-Locality.zip\nunzip vSwitch-Pipelines.zip\n</code></pre></p> <p>Step 4: rename downloaded directories to match the vars/main.yml file shell<pre><code>mv Traffic-Locality mini_profiles\nmv vSwitch-Pipelines ovs-pipelines\n</code></pre></p> <p>Now, we are ready to setup the orchestrator and install <code>gvs</code> and the traffic generator (<code>tgen</code>).</p>"},{"location":"installation/#orchestrator-setup","title":"Orchestrator Setup","text":"<p>You only need to setup the gigaflow-orchestrator repository that will bringup the testbed, install all dependencies (including <code>gvs</code> and traffic generator), and run the experiments.  The orchestration is enabled via Ansible which itself is provided as a docker container.</p> <p>Note</p> <p>All the steps from this point onwards must be run on your orchestrator machine. For our experiments, we used the <code>gvs</code> machine as our orchestrator but you can choose a different machine too as long as it has <code>docker</code> installed.</p> <p>Clone the orchestrator repository as following:</p> shell<pre><code>git clone https://github.com/gigaflow-vswitch/gigaflow-orchestrator/\n</code></pre>"},{"location":"installation/#update-local-paths","title":"Update Local Paths","text":"<p>In this repository, modify the following variables in the vars/main.yml file:</p> vars/main.yml<pre><code>retrieve:\n  caida:\n    path: \"/home/&lt;username&gt;/Gigaflow/mini_profiles\"\n  pipelines:\n    path: \"/home/&lt;username&gt;/Gigaflow/ovs-pipelines\"\n  destination: \n    path: \"/tmp/{{ project.name }}/pipelines-and-traffic\"\n</code></pre> <p>Update only the <code>retrieve.caida.path</code> and <code>retrieve.pipelines.path</code> variables to point to a Gigaflow directory on the <code>COLLECTOR</code> machine.</p>"},{"location":"installation/#inventory-configurations","title":"Inventory Configurations","text":"<p>We use Ansible to orcherstrate all experiments using the three machines.  Therefore, we require <code>root</code> access to each of them.  To populate for each machine, update the inventory.ini file as following:</p> inventory.ini<pre><code>[NODES]\nTGEN ansible_host=&lt;tgen-ip&gt; ansible_user=&lt;tgen-username&gt; ansible_password=&lt;tgen-password&gt; ansible_sudo_pass=&lt;tgen-root-password&gt;\nGVS ansible_host=&lt;ovs-ip&gt; ansible_user=&lt;ovs-username&gt; ansible_password=&lt;ovs-password&gt; ansible_sudo_pass=&lt;ovs-root-password&gt;\n\n[STORAGE]\nCOLLECTOR ansible_host=&lt;collector-ip&gt; ansible_user=&lt;collector-username&gt; ansible_password=&lt;collector-password&gt; ansible_sudo_pass=&lt;collector-root-password&gt; ansible_ssh_user=&lt;collector-username&gt; ansible_ssh_pass=&lt;collector-root-password&gt;\n</code></pre> <p>To test if all machines are reachable, run the following command:</p> shell<pre><code>cd Gigaflow-Artifact-ASPLOS2025\nmake ansible\n</code></pre> <p>This should start an <code>Ansible</code> docker container. Run the next commands from inside this container.</p> <p>Note</p> <p>Except for <code>make ansible</code>, all make targets must always be run from inside this Ansible docker container.</p> Ansible Container<pre><code>make ping\n</code></pre> <p>This should be successful and return something like this:</p> <pre><code>root@nga2-vm2:/workdir# make ping\nansible all -m ping\nCOLLECTOR | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nGVS | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nTGEN | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>This means all machines are reachable and you can now proceed to the next step.</p>"},{"location":"installation/#gvs-and-tgen-installation","title":"GvS and TGen Installation","text":"<p>Now, you can install gvs and tgen along with all their dependencies as following:</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>You can also install them separately as following:</p> Ansible Container<pre><code>make install-dataset\nmake install-gvs\nmake install-tgen\n</code></pre> <p>At this point, you have succesfully installed <code>gvs</code> on the <code>GVS</code> machine and <code>tgen</code> on the <code>TGEN</code> machine. You have also retrieved and placed your pipelines and traffic traces on both machines and now you are ready to start running experiments!</p>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<pre><code>gigaflow-orchestrator/\n\u251c\u2500\u2500 roles/          # Ansible roles for all components needed to run GvS experiments\n\u2502   \u251c\u2500\u2500 collector/\n\u2502   \u251c\u2500\u2500 dpdk/\n\u2502   \u251c\u2500\u2500 gvs/\n\u2502   \u251c\u2500\u2500 logging/\n\u2502   \u251c\u2500\u2500 retrieve/\n\u2502   \u251c\u2500\u2500 rules/\n\u2502   \u2514\u2500\u2500 tgen/\n\u251c\u2500\u2500 scripts/        \n\u251c\u2500\u2500 vars/           # Experiment variables\n\u251c\u2500\u2500 inventory.ini   # Ansible inventory file\n\u251c\u2500\u2500 ansible.cfg     # Ansible configuration file\n\u251c\u2500\u2500 Makefile        # Makefile for the ansible playbook targets\n\u251c\u2500\u2500 gvs.yml         # top-level gvs ansible playbook\n\u251c\u2500\u2500 tgen.yml        # top-level tgen ansible playbook\n\u2514\u2500\u2500 ...             # other top-level ansible playbooks\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>For detailed instructions on using Gigaflow in your testbed, please refer to our usage guide.</p> <p>To evaluate Gigaflow performance against Megaflow cache with real-world workloads, please refer to our benchmarking guide.</p>"},{"location":"installation/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Performance Benchmarks</li> <li>Usage Instructions</li> </ul>"},{"location":"publications/","title":"Publications","text":"<p>ASPLOS 2025 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>Hot Chips 2024 Gigaflow: A Smart Cache for a SmartNIC! Annus Zulfiqar, Ali Imran, Venkat Kunaparaju, Ben Pfaff, Gianni Antichi, Muhammad Shahbaz</p> <p>SIGCOMM CCR 2023 The Slow Path Needs an Accelerator Too! Annus Zulfiqar, Ben Pfaff, William Tu, Gianni Antichi, Muhammad Shahbaz</p>"},{"location":"publications/#team","title":"Team","text":"<ul> <li>Annus Zulfiqar (University of Michigan)</li> <li>Ali Imran (University of Michigan)</li> <li>Venkat Kunaparaju (Purdue University)</li> <li>Ben Pfaff (Feldera)</li> <li>Gianni Antichi (Politechnico di Milano)</li> <li>Muhammad Shahbaz (University of Michigan)</li> </ul>"},{"location":"publications/#citation","title":"Citation","text":"<p>Please cite this paper when using Gigaflow:</p> <pre><code>@inproceedings{zulfiqar2025gigaflow,\ntitle = {{Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs}},\nauthor = {Zulfiqar, Annus and Imran, Ali and Kunaparaju, Venkat and Pfaff, Ben and Antichi, Gianni and Shahbaz, Muhammad},\nbooktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},\nyear = {2025}\npublisher = {Association for Computing Machinery},\n}\n</code></pre>"},{"location":"publications/#technical-documentation","title":"Technical Documentation","text":"<p>We maintain detailed documentation about Gigaflow:</p> <ul> <li>Getting Started and Usage</li> <li>Architecture and Implementation</li> <li>Performance Analysis</li> </ul>"},{"location":"publications/#contact","title":"Contact","text":"<p>For research-related queries or collaborations:</p> <ul> <li>Email: zulfiqaa@umich.edu</li> </ul>"},{"location":"technical-details/","title":"Technical Details","text":"<p>This document describes the technical architecture and implementation details of Gigaflow.</p>"},{"location":"technical-details/#gigaflow-architecture","title":"Gigaflow Architecture","text":"<p>Gigaflow is built as a caching sub-system in the Open vSwitch.</p> Gigaflow cache in the Open vSwitch (OVS)"},{"location":"technical-details/#pipeline-aware-locality","title":"Pipeline-Aware Locality","text":"An example of Gigaflow cache in action with three packets"},{"location":"technical-details/#disjointedness-property","title":"Disjointedness Property","text":"Gigaflow captures pipeline-aware locality by capitalizing on field-level disjointedness in traversals"},{"location":"usage/","title":"Using Gigaflow","text":"<p>This guide explains how to use Gigaflow as a new caching sub-system integrated in the Open vSwitch (GvS).</p>"},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>Before using Gigaflow, ensure you have:</p> The high-level workflow to evaluate Gigaflow (GvS)"},{"location":"usage/#running-gigaflow-and-performance-evaluation","title":"Running Gigaflow and Performance Evaluation","text":"<p>To evaluate performance:</p> <ol> <li>Follow our benchmarking guide</li> <li>Use provided workloads and scripts to emulate high/low locality environments</li> <li>Benchmark Gigaflow against Megaflow cache using real-world vSwitch pipelines and traffic traces</li> </ol>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See Installation Instructions for setup details</li> <li>Review Technical Details for architecture information</li> <li>Check Benchmarks for performance evaluation guide</li> </ul>"}]}